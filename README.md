# ğŸ¤– ApBot: Robot Operation of Home Appliances by Reading User Manuals

This repository accompanies the paper **"ApBot: Robot Operation of Home Appliances by Reading User Manuals"**, and contains **open-sourced code, datasets, simulation tools, and baseline experiments** to support research on robotic operation of household appliances.

ApBot enables robots to operate **previously unseen appliances** by â€œreadingâ€ their user manuals, grounding symbolic actions to **real-world control elements**, and executing policies **robustly and interactively** with textual or visual feedback.

---

## ğŸ“¦ What's in this Repository?

The repository is organized into three main parts:

* **`code/`** â€“ Core implementation for foundation models, symbolic reasoning, visual grounding, simulation, and real-world execution. It consists of:

  * `foundation_models/` â€“ GPT, OWL, and SAM2 integration
  * `simulated/` â€“ Scripts for parsing user manuals, grounding, generating test cases, and running experiments. Includes:

    * `paper_exp/` â€“ Scripts and outputs for reproducing experiments and baselines reported in the paper
  * `real_world/` â€“ Scripts for running real-world experiments

* **`dataset/`** â€“ Structured datasets for training and evaluation. It consists of:

  * `simulated/` â€“ Includes six simulated appliances (e.g., microwave, washer), each with user manuals, control panel images, and symbolic simulators
  * `real_world/` â€“ Includes real-world recordings of five appliances, structured in the same format as the simulated data

* **`README`** â€“ Documentation and usage guide

---

## ğŸ§  Key Features

* **Structured Appliance Modeling**: Automatically extract and build symbolic models (variables, features, actions) from unstructured manuals.
* **Vision-Language Grounding**: Ground control instructions to appliance control panels using large VLMs (e.g., OWL-V2).
* **Closed-loop Execution**: Monitor execution visually and update world models based on feedback.
* **Simulated + Real-world Evaluation**: Test and benchmark baseline methods in both controlled and real scenarios.

-----

## ğŸ’» Code

This repository contains structured code, foundational models, and scripts for appliance control research. It is organized into **foundational models**, **real-world**, and **simulated** directories.

<details>
<summary>ğŸ“ <code>code/</code></summary>
The heart of the project, containing the logic for visual grounding, reasoning, and simulation.

<ul>
<details>
<summary>ğŸ“ <code>foundation_models/</code></summary>
Contains foundational models such as GPT, OWL, and SAM2.
<ul>
<li>ğŸ“„ <code>gpt_4o_model.py</code> â€“ Used to call GPT</li>
<li>ğŸ“„ <code>owlv2_crane5_api.py</code> â€“ Starts a microservice to recognize appliance panel buttons and dials</li>
<li>ğŸ“„ <code>owlv2_crane5_query.py</code> â€“ Once the API is running, functions in this script can be used to call OWLv2</li>
</ul>
</details>



<details>
<summary>ğŸ“ <code>simulated/</code></summary>
Simulation-related scripts for processing user manuals, grounding control panel elements, and generating test cases.
<ul>
<li>ğŸ“„ <code>init.py</code></li>
<li>ğŸ“„ <code>_0_t2a_config.py</code> â€“ Specifies the root code path</li>
<li>ğŸ“„ <code>_1_pdf_to_text.py</code> â€“ Converts PDF user manuals to text</li>
<li>ğŸ“„ <code>_2_extract_control_panel_element_names_from_user_manual.py</code> â€“ Extracts button/dial name lists from manuals</li>
<li>ğŸ“„ <code>_3_detect_bbox_from_photos.py</code> â€“ Detects bounding boxes from appliance panel images</li>
<li>ğŸ“„ <code>_4_map_control_panel_element_names_to_bbox_indexes.py</code> â€“ Maps button/dial names to possible bounding box indices</li>
<li>ğŸ“„ <code>_5_json_map_control_panel_element_names_to_bbox_indexes.py</code> â€“ Formats the previous mapping results into JSON</li>
<li>ğŸ“„ <code>_6_remove_duplicate_bboxes.py</code> â€“ Ensures one-to-one mapping between buttons/dials and bounding box indices</li>
<li>ğŸ“„ <code>_7_json_map_control_panel_element_names_to_bbox_indexes.py</code> â€“ (Appears redundant; formats mapping results into JSON again)</li>
<li>ğŸ“„ <code>_8_visualise_grounding_control_element_name_result.py</code> â€“ Generates visualization images showing all buttons/dials as labeled bounding boxes</li>
<li>ğŸ“„ <code>_10_propose_action_names.py</code> â€“ Proposes actions based on user manuals</li>
<li>ğŸ“„ <code>_11_generated_grounded_action_json.py</code> â€“ Maps proposed actions to bounding box coordinates</li>
<li>ğŸ“„ <code>_12_match_proposed_action_to_oracle_action.py</code> â€“ Checks if proposed actions match oracle execution regions using bounding box coordinates</li>
<li>ğŸ“„ <code>_13_visualisation_grounding_action_result.py</code> â€“ Generates visualization images displaying grounded actions as labeled bounding boxes</li>
<li>ğŸ“„ <code>_18_generate_testcases.py</code> â€“ (Deprecated) Generates ambiguous instructions that can be satisfied by one or multiple policies</li>
<li>ğŸ“„ <code>_18_generate_testcases_v2.py</code> â€“ Generates instructions requiring different step sizes to achieve goals</li>
<li>ğŸ“„ <code>_19_generate_target_state.py</code> â€“ For each test instruction, generates the target state in the appliance simulator</li>

<details>
<summary>ğŸ“ <code>samples/</code></summary>
Used as example appliance models.
<ul>
<li>ğŸ“„ <code>_0_logic_units.py</code> â€“ Appliance model templates, combines the content of variables, features and actions</li>
<li>ğŸ“„ <code>_0_sample_machine.py</code> â€“ Example appliance models</li>
<li>ğŸ“„ <code>_0_search.py</code> â€“ Macro action logics</li>
<li>ğŸ“„ <code>_1_variables.py</code> â€“ Appliance variable templates</li>
<li>ğŸ“„ <code>_2_features.py</code> â€“ Appliance feature templates, used to specify the macro actions</li>
<li>ğŸ“„ <code>_3_actions.py</code> â€“ Appliance dynamics templates, used to specify action effects</li>
</ul>
</details>

<details>
<summary>ğŸ“ <code>prompts/</code></summary>
Stores all the prompts used.
<ul>
<li>ğŸ“„ <code>various prompt text file:</code> â€“ Is used for the python files in the simulated/ folder</li>
<li>ğŸ“„ <code>paper_exp/baselines_v1/:</code> â€“ Is used for various baselines in the paper. _4_HV_M_SR_MA_agnostic refers to the ApBot algorithm.</li>
<li>ğŸ“„ <code>_17_testcase/:</code> â€“ The prompt used to generate instructions that are ambiguous or specific</li>
<li>ğŸ“„ <code>_17_testcase_v2/:</code> â€“ The prompt used to generate instructions that require different numbers of steps to finish</li>
</ul>
</details>

<details>
<summary>ğŸ“ <code>paper_exp/</code></summary>
<ul>
<li>
<details>
<summary>ğŸ“ </code>baselines_v1/</code></summary>
<ul>
<li>ğŸ“„ code for running paper baselines. ApBot has the file name _4_M_SR_MA_agnostic. </li>
</ul>
</details>
</li>
<li>
<details>
<summary>ğŸ“ </code>real_world/</code></summary>
<ul>
<li>ğŸ“„ code for running real world experiments</li>
</ul>
</details>
</li>
</ul>
</details>

<details>
<summary>ğŸ“ <code>utils/</code></summary>
<ul>
<li> Some helper functions to load files.
<li> <details>
<summary>ğŸ“ <code>task/</code></summary>
<ul>
<li>ğŸ“„ <code>calibrate_current_value.py</code> â€“ During close loop execution, if the display is an unexpected value, calibrates the appliance model according to predefined routines and templates</li>
<li>ğŸ“„ <code>check_reasoning_file_existance.py</code> â€“ Checks file existence</li>
<li>ğŸ“„ <code>check_state_reached.py</code> â€“ Checks if the appliance state has reached those required by an instruction</li>
<li>ğŸ“„ <code>compare_visual_grounding.py</code> â€“ Compares the results between ApBot and Molmo visual grounding performance</li>
<li>ğŸ“„ <code>derive_variable_definition.py</code> â€“ Helper function to help update the variable in the appliance model when there is a feedback mismatch</li>
<li>ğŸ“„ <code>generate_report.py</code> â€“ After baseline results are out, use it to generate instruction execution results</li>
<li>ğŸ“„ <code>generate_updated_goal.py</code> â€“ Used to update the ApBot appliance model target state</li>
<li>ğŸ“„ <code>interact_with_simulator.py</code> â€“ Used for ApBot to interact with appliance simulators</li>
<li>ğŸ“„ <code>mathces_run_action_format.py</code> â€“ Used to check command formats used by ApBot to interact with appliance simulators.</li>
<li>ğŸ“„ <code>prepare_simulator.py</code> â€“ Used to load appliance simulators</li>
<li>ğŸ“„ <code>propose_actions.py</code> â€“ During close-loop execution, ApBot proposes a suitable action given the current appliance state</li>
<li>ğŸ“„ <code>propose_feature_list.py</code> â€“ Proposes macro actions of appliance models</li>
<li>ğŸ“„ <code>propose_goal_state.py</code> â€“ Proposes the appliance model's target state</li>
<li>ğŸ“„ <code>propose_variables.py</code> â€“ Proposes appliance model variables</li>
<li>ğŸ“„ <code>propose_world_model_agnostic_to_command.py</code> â€“ Proposes the appliance model's action dynamics</li>
</ul>
</details>
</ul>
</details>

</ul>
</details>

<details>
<summary>ğŸ“ <code>real_world/</code></summary>
Scripts for real-world execution. Code structure is the combined content of the dataset folder and the simulated code folder.
</details>

</details>
</ul>

-----

## ğŸ’¾ Appliance Simulation Dataset

This repository contains structured datasets, simulators, and baseline experiment results for appliance control research. It is organized into **simulated** and **real-world** datasets.

### 1\. Simulated

The simulated dataset includes data for the following appliances, with **5 instances** each:

  * Dehumidifier
  * Bottle Washer
  * Rice Cooker
  * Microwave Oven
  * Bread Maker
  * Washing Machine

<details>
<summary>ğŸ“ <code>simulated/</code></summary>
The root directory for all simulated appliance data.

<ul>
<details>
<summary>ğŸ“ <code>0_websites/</code></summary>
Appliance panel images and user manuals (source websites).
</details>

<details>
<summary>ğŸ“ <code>1_user_manual/</code></summary>
Contains different forms of user manuals (PDF, image, text, and parsed elements).
<ul>
<li>ğŸ“ <strong>0_pdf/</strong> â€“ Raw PDF manuals, manually added</li>
<li>ğŸ“ <strong>1_image/</strong> â€“ Image versions of manuals, generated via code</li>
<li>ğŸ“ <strong>2_text/</strong> â€“ Extracted text from PDFs, generated via code</li>
<li>ğŸ“ <strong>3_extracted_control_panel_element_names/</strong> â€“ Control panel element names extracted from manuals, generated via code</li>
</ul>
</details>

<details>
<summary>ğŸ“ <code>2_control_panel_images/</code></summary>
Control panel images and grounded elements.
<ul>
<li>ğŸ“ <strong>0_raw/</strong> â€“ Images copied from websites, manually added</li>
<li>ğŸ“ <strong>1_selected/</strong> â€“ Final selected single image, manually added</li>
<li>
<details>
<summary>ğŸ“ 2_ground_control_panel_elements/</summary>
<ul>
<li>ğŸ“ <strong>1_validity_control_panel/</strong> â€“ One bounding box per image circling a button or dial</li>
<li>ğŸ“ <strong>2_bboxes_on_control_panel/</strong> â€“ JSON files of button/dial bounding boxes</li>
<li>ğŸ“ <strong>3_bboxes_on_control_panel_visualisation/</strong> â€“ Visual summary of all bounding boxes</li>
<li>ğŸ“ <strong>4_query_images_bbox_to_name/</strong> â€“ Red box for query, green boxes as references</li>
<li>ğŸ“ <strong>5_query_images_unique_bbox_id/</strong> â€“ Indexed candidate boxes per button/dial</li>
</ul>
</details>
</li>
</ul>
</details>

<details>
<summary>ğŸ“ <code>3_simulators/</code></summary>
Contains simulators and their associated assets.
<ul>
<li>ğŸ“ <strong>0_control_panel_images_groundtruth_annotation/</strong> â€“ COCO annotations (manual)</li>
<li>ğŸ“ <strong>1_oracle_available_actions/</strong> â€“ Oracle action lists (manual)</li>
<li>ğŸ“ <strong>2_map_oracle_action_to_annotation_label/</strong> â€“ Dict: action names â†’ labels + types (manual)</li>
<li>ğŸ“ <strong>3_oracle_simulator_action_to_bbox_mapping/</strong> â€“ Auto: action names â†’ bbox coords</li>
<li>ğŸ“ <strong>4_simulators_with_states_and_display/</strong> â€“ Sim: action â†’ text display (manual)</li>
<li>
<details>
<summary>ğŸ“ 5_testcases/</summary>
<ul>
<li>ğŸ“ <strong>1_testcases_var_raw/</strong> â€“ Generated task instructions</li>
<li>ğŸ“ <strong>2_testcases_var/</strong> â€“ Selected instructions (manual)</li>
<li>ğŸ“ <strong>3_testcases_var_with_target_states/</strong> â€“ Target states (generated)</li>
<li>ğŸ“ <strong>4_testcases_var_formatted/</strong> â€“ Generated + manually corrected</li>
</ul>
</details>
</li>
</ul>
</details>

<details>
<summary>ğŸ“ <code>4_visual_grounding/</code></summary>
Visual grounding data for control panel elements and action names.
<ul>
<li>
<details>
<summary>ğŸ“ 0_control_panel_element_bbox/</summary>
<ul>
<li>ğŸ“ <strong>0_control_panel_element_index/</strong> â€“ Bbox index â†’ element names</li>
<li>ğŸ“ <strong>1_control_panel_element_index_json/</strong> â€“ Element names â†’ bbox indices</li>
<li>ğŸ“ <strong>2_control_panel_element_index_json_unique/</strong> â€“ Unique name â†’ bbox index</li>
<li>ğŸ“ <strong>3_proposed_control_panel_element_bbox/</strong> â€“ Element name â†’ coordinates</li>
<li>ğŸ“ <strong>4_visualised_proposed_control_panel_element_bbox/</strong> â€“ Indexed bbox visualization</li>
<li>ğŸ“ <strong>5_visualised_localised_buttons_no_label/</strong> â€“ Center-indexed bbox visualizations</li>
<li>ğŸ“ <strong>6_visualised_localised_buttons_legends/</strong> â€“ Bbox index â†’ coordinates</li>
<li>ğŸ“ <strong>7_proposed_buttons_to_oracle_action_mapping/</strong> â€“ Proposed bbox â†’ oracle actions</li>
<li>ğŸ“ <strong>9_extracted_control_panel_labels/</strong> â€“ Element names from manual</li>
</ul>
</details>
</li>
<li>
<details>
<summary>ğŸ“ 1_action_names/</summary>
<ul>
<li>ğŸ“ <strong>1_proposed_action_names/</strong> â€“ Proposed actions</li>
<li>ğŸ“ <strong>2_proposed_world_model_action_bbox/</strong> â€“ Action name â†’ bbox</li>
<li>ğŸ“ <strong>3_proposed_to_oracle_action_mapping/</strong> â€“ Action name â†’ oracle mapping (ApBot)</li>
<li>ğŸ“ <strong>4_molmo_proposed_action_coords/</strong> â€“ Action name â†’ image coords</li>
<li>ğŸ“ <strong>5_molmo_proposed_actions_visualisation/</strong> â€“ Visualized action positions</li>
<li>ğŸ“ <strong>6_molmo_proposed_to_oracle_action_mapping/</strong> â€“ Action name â†’ oracle mapping (Molmo)</li>
</ul>
</details>
</li>
</ul>
</details>

<details>
<summary>ğŸ“ <code>6_results/</code></summary>
<ul>
<li>ğŸ“ <strong>1_visual_grounding_action_results/</strong> â€“ Metrics for action visual grounding</li>
<li>ğŸ“ <strong>3_visualize_proposed_actions/</strong> â€“ Visual results of grounded actions</li>
<li>
<details>
<summary>ğŸ“ 6_paper_exp/</summary>
<ul>
<li><strong>Baseline experiment results</strong></li>
<li>ğŸ“ <strong>11_visualisation/</strong> â€“ Baseline output visualizations</li>
</ul>
</details>
</li>
</ul>
</details>
</details>
</ul>

-----

### 2\. Real World

The real-world dataset includes data for the following appliances, with **1 instance** each:

  * Washing Machine
  * Rice Cooker
  * Blender
  * Water Dispenser
  * Induction Cooker

The internal folder structure is identical to the simulated dataset.

-----

## ğŸ§ª Baseline Experiments

The baseline experiments evaluate different combinations of **perception**, **appliance model**, **reasoning**, and **policy**.

| ID | Name | Visual Grounding (Perception) | Have Access To User Manual | Reasoning | Action Policy | Name in Paper |
| :-- | :---------------------- | :---------------------------- | :--------------- | :------------ | :----------------- | :------------------------------------------------ |
| 1 | NV_M_UR_LP | Oracle | Yes | Unstructured | LLM | LLM as policy w/ grounded actions |
| 2 | HV_M_UR_LP | Proposed | Yes | Unstructured | LLM | LLM as policy w/ image |
| 4 | M_SR_MA | Proposed | Yes | Structured | Macro-actions | **ApBot** |
| 5 | HV_M_SR_LP | Proposed | Yes | Structured | LLM | ApBot w/o button policy |
| 7 | HV_M_UR_MA | Proposed | Yes | Unstructured | Macro-actions | ApBot w/o model |
| 8 | HV_M_SR_MA_OL | Proposed | Yes | Structured | Macro-actions | ApBot w/o close-loop update |
| 9 | oracle_V_oracle_M | Oracle | Yes | Structured & Oracle | Macro-actions | N.A |
| 10 | oracle_V_proposed_M | Oracle | Yes | Structured | Macro-actions | N.A |

-----

## ğŸ“Š Results

  * **Visual Grounding Metrics**: Stored in `dataset/simulated/6_results/1_visual_grounding_action_results/`
  * **Action Visualizations**: Stored in `dataset/simulated/6_results/3_visualize_proposed_actions/`
  * **Baseline Experiment Outputs**: Stored in `dataset/simulated/6_results/6_paper_exp/`
  * **Comparative Visualisations**: Stored in `dataset/simulated/6_results/6_paper_exp/11_visualisation/`

-----

## ğŸš€ Usage

To understand and run the full ApBot pipeline, start from the **`code/simulated/`** directory. The pipeline includes **reading PDFs**, **visual grounding**, **building symbolic models**, and **testing the model**. Below is the step-by-step process:

---

### 1. ğŸ“˜ Read and Parse User Manuals

Convert appliance manuals from PDF to structured elements.

| Step | Description                                   | Script                                                       |
| ---- | --------------------------------------------- | ------------------------------------------------------------ |
| 1.1  | Convert PDF manuals into raw text             | `_1_pdf_to_text.py`                                          |
| 1.2  | Extract control panel element names from text | `_2_extract_control_panel_element_names_from_user_manual.py` |

---

### 2. ğŸ–¼ï¸ Perform Visual Grounding

Use control panel images and align them with textual instructions.

| Step | Description                                 | Script                                                                                                                     |
| ---- | ------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| 2.1  | Detect bounding boxes from panel images     | `_3_detect_bbox_from_photos.py`                                                                                            |
| 2.2  | Map detected boxes to control element names | `_4_map_control_panel_element_names_to_bbox_indexes.py`                                                                    |
| 2.3  | Format mappings into structured JSON        | `_5_json_map_control_panel_element_names_to_bbox_indexes.py`, `_7_json_map_control_panel_element_names_to_bbox_indexes.py` |
| 2.4  | Remove duplicate or conflicting mappings    | `_6_remove_duplicate_bboxes.py`                                                                                            |
| 2.5  | Visualize grounding results                 | `_8_visualise_grounding_control_element_name_result.py`                                                                    |

---

### 3. ğŸ§  Build the Appliance World Model

Generate appliance variables, features, and macro-actions.

| Step | Description                                   | Script                                          |
| ---- | --------------------------------------------- | ----------------------------------------------- |
| 3.1  | Propose actions from manuals                  | `_10_propose_action_names.py`                   |
| 3.2  | Map actions to bounding boxes                 | `_11_generated_grounded_action_json.py`         |
| 3.3  | Match proposed actions to oracle ground-truth | `_12_match_proposed_action_to_oracle_action.py` |
| 3.4  | Visualize grounded actions                    | `_13_visualisation_grounding_action_result.py`  |

---

### 4. ğŸ§ª Test Using Simulated Instructions

Generate test instructions and target appliance states.

| Step | Description                                  | Script                         |
| ---- | -------------------------------------------- | ------------------------------ |
| 4.1  | Generate ambiguous or multistep instructions | `_18_generate_testcases_v2.py` |
| 4.2  | Generate target states for each test         | `_19_generate_target_state.py` |

---

### 5. ğŸ§¹ Run ApBot (Symbolic Reasoning + Policy Execution)

Run the main ApBot experiment using the generated appliance model and test instructions.

| Step | Description                                                                  | Script                                          |
| ---- | ---------------------------------------------------------------------------- | ----------------------------------------------- |
| 5.1  | Main file for symbolic reasoning and macro-action execution (ApBot pipeline) | `paper_exp/baselines_v1/_4_M_SR_MA_agnostic.py` |

---

### ğŸ› ï¸ Optional: World Model Construction and Reasoning Helpers

Located in `code/utils/task/`, these helper scripts support reasoning, calibration, and simulator interaction during closed-loop execution. For example:

* `interact_with_simulator.py` â€“ ApBot interacting with simulator
* `generate_updated_goal.py` â€“ Update goals dynamically based on feedback
* `propose_goal_state.py` â€“ Generate target states
* `check_state_reached.py` â€“ Check if state matches the goal
* `calibrate_current_value.py` â€“ Adjust model if observed feedback is off